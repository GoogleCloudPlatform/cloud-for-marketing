{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import kfp\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, ClassificationMetrics, Metrics, component)\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import storage\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "from typing import NamedTuple\n",
    "\n",
    "# We'll use this beta library for metadata querying\n",
    "from google.cloud import aiplatform_v1beta1\n",
    "from datetime import datetime"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "PATH=%env PATH\n",
    "%env PATH={PATH}:/home/jupyter/.local/bin\n",
    "REGION=\"us-central1\"\n",
    "\n",
    "PIPELINE_ROOT = 'gs://propensity_assets/pipeline_root'\n",
    "PIPELINE_ROOT"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "env: PATH=/opt/conda/bin:/opt/conda/condabin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games:/home/jupyter/.local/bin\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'gs://propensity_assets/pipeline_root'"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "VIEW_NAME = 'ga_data' # BigQuery view you create for input data to model\n",
    "DATA_SET_ID = 'propensity' # The Data Set ID where the view sits\n",
    "PROJECT_ID = '' # The Project ID\n",
    "BUCKET_NAME = '' # Bucket where the base_sql.txt file lives\n",
    "BLOB_PATH = '' # The actual path where base_sql will be sent to"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# In order to build BQ Dataset\n",
    "!gcloud config set project $PROJECT_ID\n",
    "REGION = 'US'\n",
    "bq mk --location=$REGION --dataset $PROJECT_ID:$DATA_SET_ID"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Send base_sql.txt to GCS bucket\n",
    "\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.get_bucket(BUCKET_NAME)\n",
    "blob = bucket.blob(BLOB_PATH)\n",
    "blob.upload_from_filename(\"kfp/base_sql.txt\")\n",
    "blob.public_url"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'https://storage.googleapis.com/taitest/tai_test_pipeline/base_sql.txt'"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "@component(\n",
    "    # this component builds a BQ view, which will be the underlying source for model\n",
    "    packages_to_install=[\"google-cloud-bigquery\", \"google-cloud-storage\"],\n",
    "    base_image=\"python:3.9\",\n",
    "    output_component_file=\"output_component/create_input_view.yaml\",\n",
    ")\n",
    "\n",
    "def create_input_view(view_name: str, \n",
    "                      data_set_id: str, \n",
    "                      project_id: str,\n",
    "                      bucket_name: str,\n",
    "                      blob_path: str\n",
    "                    \n",
    "):\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import storage\n",
    "    client = bigquery.Client(project=project_id)\n",
    "    dataset = client.dataset(data_set_id)\n",
    "    table_ref = dataset.table(view_name)\n",
    "    ga_data_ref = 'bigquery-public-data.google_analytics_sample.ga_sessions_*'\n",
    "    conversion = \"hits.page.pageTitle like '%Shopping Cart%'\" # this is sql like syntax used to define the conversion in the GA360 raw export\n",
    "    start_date = '20170101'\n",
    "    end_date = '20170131'\n",
    "\n",
    "    \n",
    "    def get_sql(bucket_name, blob_path):\n",
    "        from google.cloud import storage\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.get_bucket(bucket_name)\n",
    "        blob = bucket.get_blob(blob_path)\n",
    "        content = blob.download_as_string()\n",
    "        return content\n",
    "\n",
    "    def if_tbl_exists(client, table_ref):\n",
    "        from google.cloud.exceptions import NotFound\n",
    "        try:\n",
    "            client.get_table(table_ref)\n",
    "            return True\n",
    "        except NotFound:\n",
    "            return False\n",
    "\n",
    "    if if_tbl_exists(client, table_ref):\n",
    "        print(\"view already exists\")\n",
    "        \n",
    "    else: \n",
    "        #load sql from base_sql.txt.  This can be modified if you want to modify your query\n",
    "        content = get_sql(bucket_name, blob_path)\n",
    "        content = str(content, 'utf-8')\n",
    "        create_base_feature_set_query = content.format(start_date = start_date,\n",
    "                                                       end_date = end_date, \n",
    "                                                       ga_data_ref = ga_data_ref, \n",
    "                                                       conversion = conversion)\n",
    "\n",
    "        shared_dataset_ref = client.dataset(data_set_id)\n",
    "        base_feature_set_view_ref = shared_dataset_ref.table(view_name)\n",
    "        base_feature_set_view = bigquery.Table(base_feature_set_view_ref)\n",
    "        base_feature_set_view.view_query = create_base_feature_set_query.format(project_id)\n",
    "        base_feature_set_view = client.create_table(base_feature_set_view)  # API request"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "@component(\n",
    "    # this component builds a logistic regression with BQML\n",
    "    packages_to_install=[\"google-cloud-bigquery\"],\n",
    "    base_image=\"python:3.9\",\n",
    "    output_component_file=\"output_component/create_bqml_model_logistic.yaml\"\n",
    ")\n",
    "\n",
    "\n",
    "def build_bqml_logistic(project_id: str, \n",
    "                         data_set_id: str, \n",
    "                         model_name: str, \n",
    "                         training_view: str\n",
    "):\n",
    "    from google.cloud import bigquery\n",
    "    client = bigquery.Client(project=project_id)\n",
    "\n",
    "    model_name = f\"{project_id}.{data_set_id}.{model_name}\"\n",
    "    training_set = f\"{project_id}.{data_set_id}.{training_view}\"\n",
    "    build_model_query_bqml_logistic = '''\n",
    "    CREATE OR REPLACE MODEL `{model_name}`\n",
    "    OPTIONS(model_type='logistic_reg'\n",
    "    , INPUT_LABEL_COLS = ['label']\n",
    "    , L1_REG = 1\n",
    "    , DATA_SPLIT_METHOD = 'RANDOM'\n",
    "    , DATA_SPLIT_EVAL_FRACTION = 0.20\n",
    "    ) AS\n",
    "        SELECT * EXCEPT (fullVisitorId, label), \n",
    "        CASE WHEN label is null then 0 ELSE label end as label\n",
    "    FROM `{training_set}`\n",
    "    '''.format(model_name = model_name, training_set = training_set)\n",
    "\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    client.query(build_model_query_bqml_logistic, job_config=job_config)  # Make an API request."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "@component(\n",
    "    # this component builds an xgboost classifier with BQML\n",
    "    packages_to_install=[\"google-cloud-bigquery\"],\n",
    "    base_image=\"python:3.9\",\n",
    "    output_component_file=\"output_component/create_bqml_model_xgboost.yaml\"\n",
    ")\n",
    "\n",
    "\n",
    "def build_bqml_xgboost(project_id: str, \n",
    "                         data_set_id: str, \n",
    "                         model_name: str, \n",
    "                         training_view: str\n",
    "):\n",
    "    from google.cloud import bigquery\n",
    "    client = bigquery.Client(project=project_id)\n",
    "\n",
    "    model_name = f\"{project_id}.{data_set_id}.{model_name}\"\n",
    "    training_set = f\"{project_id}.{data_set_id}.{training_view}\"\n",
    "    build_model_query_bqml_xgboost = '''\n",
    "    CREATE OR REPLACE MODEL `{model_name}`\n",
    "    OPTIONS(model_type='BOOSTED_TREE_CLASSIFIER'\n",
    "    , INPUT_LABEL_COLS = ['label']\n",
    "    , L1_REG = 1\n",
    "    , DATA_SPLIT_METHOD = 'RANDOM'\n",
    "    , DATA_SPLIT_EVAL_FRACTION = 0.20\n",
    "    ) AS\n",
    "        SELECT * EXCEPT (fullVisitorId, label), \n",
    "        CASE WHEN label is null then 0 ELSE label end as label\n",
    "    FROM `{training_set}`\n",
    "    '''.format(model_name = model_name, training_set = training_set)\n",
    "\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    client.query(build_model_query_bqml_xgboost, job_config=job_config)  # Make an API request."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "@component(\n",
    "    # this component builds an AutoML classifier with BQML\n",
    "    packages_to_install=[\"google-cloud-bigquery\"],\n",
    "    base_image=\"python:3.9\",\n",
    "    output_component_file=\"output_component/create_bqml_model_automl.yaml\"\n",
    ")\n",
    "\n",
    "\n",
    "def build_bqml_automl(project_id: str, \n",
    "                         data_set_id: str, \n",
    "                         model_name: str, \n",
    "                         training_view: str\n",
    "):\n",
    "    from google.cloud import bigquery\n",
    "    client = bigquery.Client(project=project_id)\n",
    "\n",
    "    model_name = f\"{project_id}.{data_set_id}.{model_name}\"\n",
    "    training_set = f\"{project_id}.{data_set_id}.{training_view}\"\n",
    "    build_model_query_bqml_automl = '''\n",
    "    CREATE OR REPLACE MODEL `{model_name}`\n",
    "    OPTIONS(model_type='BOOSTED_TREE_CLASSIFIER'\n",
    "    , INPUT_LABEL_COLS = ['label']\n",
    "    ) AS\n",
    "        SELECT * EXCEPT (fullVisitorId, label), \n",
    "        CASE WHEN label is null then 0 ELSE label end as label\n",
    "    FROM `{training_set}`\n",
    "    '''.format(model_name = model_name, training_set = training_set)\n",
    "\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    client.query(build_model_query_bqml_automl, job_config=job_config)  # Make an API request."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "@component(\n",
    "    # this component builds an xgboost classifier with xgboost\n",
    "    packages_to_install=[\"google-cloud-bigquery\", \"xgboost\", \"pandas\", \"sklearn\", \"joblib\", \"pyarrow\"],\n",
    "    base_image=\"python:3.9\",\n",
    "    output_component_file=\"output_component/create_xgb_model_xgboost.yaml\"\n",
    ")\n",
    "\n",
    "def build_xgb_xgboost(project_id: str, \n",
    "                            data_set_id: str, \n",
    "                            training_view: str,\n",
    "                            metrics: Output[Metrics],\n",
    "                            model: Output[Model]\n",
    "\n",
    "):\n",
    "    from google.cloud import bigquery\n",
    "    import xgboost as xgb\n",
    "    from xgboost import XGBClassifier\n",
    "    from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV, GridSearchCV\n",
    "    from sklearn.metrics import accuracy_score, roc_auc_score, precision_recall_curve\n",
    "    from joblib import dump\n",
    "    import pandas as pd\n",
    "    import pyarrow\n",
    "\n",
    "    client = bigquery.Client(project=project_id)\n",
    "\n",
    "    data_set = f\"{project_id}.{data_set_id}.{training_view}\"\n",
    "    build_df_for_xgboost = '''\n",
    "    SELECT * FROM `{data_set}`\n",
    "    '''.format(data_set = data_set)\n",
    "\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    df = client.query(build_df_for_xgboost, job_config=job_config).to_dataframe()  # Make an API request.\n",
    "    df = pd.get_dummies(df.drop(['fullVisitorId'], axis=1), prefix=['visited_dma', 'visited_daypart', 'visited_dow'])\n",
    "\n",
    "\n",
    "    X = df.drop(['label'], axis=1).values\n",
    "    y = df['label'].values\n",
    "\n",
    "    X_train, X_test, y_train, y_test  = train_test_split(X,y)\n",
    "    train = xgb.DMatrix(X_train, label=y_train)\n",
    "    test = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "    params = {\n",
    "        'reg_lambda':[0,1],\n",
    "        'gamma': [1, 1.5, 2, 2.5, 3],\n",
    "        'max_depth':[2,3,4,5,10,20],\n",
    "        'learning_rate': [.1,.01]\n",
    "\n",
    "    }\n",
    "\n",
    "    xgb_model = XGBClassifier(n_estimators=50, objective='binary:hinge',\n",
    "                              silent=True, nthread=1,\n",
    "                              eval_metric=\"auc\")\n",
    "\n",
    "    folds = 3\n",
    "    param_comb = 5\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n",
    "\n",
    "    random_search = RandomizedSearchCV(xgb_model, param_distributions=params, \n",
    "                                       n_iter=param_comb, scoring='precision', \n",
    "                                       n_jobs=4, cv=skf.split(X_train,y_train), verbose=3, \n",
    "                                       random_state=1001 )\n",
    "\n",
    "\n",
    "    random_search.fit(X_train, y_train)\n",
    "    xgb_model_best = random_search.best_estimator_\n",
    "    predictions = xgb_model_best.predict(X_test)\n",
    "    score = accuracy_score(y_test, predictions)\n",
    "    auc = roc_auc_score(y_test, predictions)\n",
    "    precision_recall = precision_recall_curve(y_test, predictions)\n",
    "    \n",
    "    metrics.log_metric(\"accuracy\",(score * 100.0))\n",
    "    metrics.log_metric(\"framework\", \"xgboost\")\n",
    "    metrics.log_metric(\"dataset_size\", len(df))\n",
    "    metrics.log_metric(\"AUC\", auc)\n",
    "    \n",
    "    dump(xgb_model_best, model.path + \".joblib\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "@component(\n",
    "    # this component evaluations Logistic Regression\n",
    "    packages_to_install=[\"google-cloud-bigquery\", \"pandas\", \"pyarrow\", \"matplotlib\"],\n",
    "    base_image=\"python:3.9\",\n",
    "    output_component_file=\"output_component/evaluate_bqml_model_logistic.yaml\"\n",
    ")\n",
    "\n",
    "\n",
    "def evaluate_bqml_logistic(project_id: str, \n",
    "                            data_set_id: str, \n",
    "                            model_name: str, \n",
    "                            training_view: str,\n",
    "                            logistic_data_path: OutputPath(\"Dataset\")\n",
    "):\n",
    "    from google.cloud import bigquery\n",
    "    import pandas as pd\n",
    "    import pyarrow\n",
    "    import matplotlib as plt\n",
    "    client = bigquery.Client(project=project_id)\n",
    "\n",
    "    model_name = project_id+'.'+data_set_id+'.'+model_name\n",
    "    training_set = project_id+'.'+data_set_id+'.'+training_view\n",
    "    evaluate_model_query_bqml_logistic = '''    \n",
    "    SELECT\n",
    "      round(threshold, 2) as threshold,\n",
    "      * except(threshold), \n",
    "      true_positives / (true_positives + false_positives) AS precision\n",
    "    FROM\n",
    "      ML.ROC_CURVE(MODEL `{model_name}`,\n",
    "                   TABLE `{table_name}`,\n",
    "                   GENERATE_ARRAY(0,1, 0.01))\n",
    "\n",
    "    ORDER BY threshold\n",
    "    '''.format(model_name = model_name, table_name = training_set)\n",
    "    \n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    query_job = client.query(evaluate_model_query_bqml_logistic, job_config=job_config)  # Make an API request.\n",
    "    df_evaluation_logistic = query_job.result()\n",
    "    df_evaluation_logistic = df_evaluation_logistic.to_dataframe()\n",
    "    df_evaluation_logistic.to_csv(logistic_data_path)\n",
    "    graph = df_evaluation_logistic.plot(x='threshold', y=['precision', 'recall']).get_figure()\n",
    "    graph.savefig(logistic_data_path)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "@component(\n",
    "    # this component evaluates BQML xgboost\n",
    "    packages_to_install=[\"google-cloud-bigquery\", \"pandas\", \"pyarrow\", \"matplotlib\"],\n",
    "    base_image=\"python:3.9\",\n",
    "    output_component_file=\"output_component/evaluate_bqml_model_xgboost.yaml\"\n",
    ")\n",
    "\n",
    "\n",
    "def evaluate_bqml_xgboost(project_id: str, \n",
    "                            data_set_id: str, \n",
    "                            model_name: str, \n",
    "                            training_view: str,\n",
    "                            xgboost_data_path: OutputPath(\"Dataset\")\n",
    "):\n",
    "    from google.cloud import bigquery\n",
    "    import pandas as pd\n",
    "    import pyarrow\n",
    "    import matplotlib as plt\n",
    "    client = bigquery.Client(project=project_id)\n",
    "\n",
    "    model_name = f\"{project_id}.{data_set_id}.{model_name}\"\n",
    "    training_set = f\"{project_id}.{data_set_id}.{training_view}\"\n",
    "    evaluate_model_query_bqml_xgboost = '''    \n",
    "    SELECT\n",
    "      round(threshold, 2) as threshold,\n",
    "      * except(threshold), \n",
    "      true_positives / (true_positives + false_positives) AS precision\n",
    "    FROM\n",
    "      ML.ROC_CURVE(MODEL `{model_name}`,\n",
    "                   TABLE `{table_name}`,\n",
    "                   GENERATE_ARRAY(0,1, 0.01))\n",
    "\n",
    "    ORDER BY threshold\n",
    "    '''.format(model_name = model_name, table_name = training_set)\n",
    "    \n",
    "\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    query_job = client.query(evaluate_model_query_bqml_xgboost, job_config=job_config)  # Make an API request.\n",
    "    df_evaluation_xgboost = query_job.result()\n",
    "    df_evaluation_xgboost = df_evaluation_xgboost.to_dataframe()\n",
    "    df_evaluation_xgboost.to_csv(xgboost_data_path)\n",
    "    graph = df_evaluation_xgboost.plot(x='threshold', y=['precision', 'recall']).get_figure()\n",
    "    graph.savefig(xgboost_data_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "@component(\n",
    "    # this component evaluates BQML autoML\n",
    "    packages_to_install=[\"google-cloud-bigquery\", \"pandas\", \"pyarrow\", \"matplotlib\"],\n",
    "    base_image=\"python:3.9\",\n",
    "    output_component_file=\"output_component/evaluate_bqml_model_automl.yaml\"\n",
    ")\n",
    "\n",
    "\n",
    "def evaluate_bqml_automl(project_id: str, \n",
    "                            data_set_id: str, \n",
    "                            model_name: str, \n",
    "                            training_view: str,\n",
    "                            automl_data_path: OutputPath(\"Dataset\")\n",
    "):\n",
    "    from google.cloud import bigquery\n",
    "    import pandas as pd\n",
    "    import pyarrow\n",
    "    import matplotlib as plt\n",
    "    client = bigquery.Client(project=project_id)\n",
    "\n",
    "    model_name = f\"{project_id}.{data_set_id}.{model_name}\"\n",
    "    training_set = f\"{project_id}.{data_set_id}.{training_view}\"\n",
    "    evaluate_model_query_bqml_automl = '''    \n",
    "    SELECT\n",
    "      round(threshold, 2) as threshold,\n",
    "      * except(threshold), \n",
    "      true_positives / (true_positives + false_positives) AS precision\n",
    "    FROM\n",
    "      ML.ROC_CURVE(MODEL `{model_name}`,\n",
    "                   TABLE `{table_name}`,\n",
    "                   GENERATE_ARRAY(0,1, 0.01))\n",
    "\n",
    "    ORDER BY threshold\n",
    "    '''.format(model_name = model_name, table_name = training_set)\n",
    "    \n",
    "\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    query_job = client.query(evaluate_model_query_bqml_automl, job_config=job_config)  # Make an API request.\n",
    "    df_evaluation_automl = query_job.result()\n",
    "    df_evaluation_automl = df_evaluation_automl.to_dataframe()\n",
    "    df_evaluation_automl.to_csv(automl_data_path)\n",
    "    graph = df_evaluation_automl.plot(x='threshold', y=['precision', 'recall']).get_figure()\n",
    "    graph.savefig(automl_data_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "@component(\n",
    "    # Deploys xgboost model \n",
    "    packages_to_install=[\"google-cloud-aiplatform\", \"joblib\", \"sklearn\", \"xgboost\"],\n",
    "    base_image=\"python:3.9\",\n",
    "    output_component_file=\"output_component/xgboost_deploy_component.yaml\",\n",
    ")\n",
    "def deploy_xgb(\n",
    "    model: Input[Model],\n",
    "    project_id: str,\n",
    "    vertex_endpoint: Output[Artifact],\n",
    "    vertex_model: Output[Model]\n",
    "):\n",
    "    from google.cloud import aiplatform\n",
    "    aiplatform.init(project=project_id)\n",
    "    deployed_model = aiplatform.Model.upload(\n",
    "        display_name=\"tai-propensity-test-pipeline\",\n",
    "        artifact_uri = model.uri.replace(\"model\", \"\"),\n",
    "        serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/xgboost-cpu.1-4:latest\"\n",
    "    )\n",
    "    endpoint = deployed_model.deploy(machine_type=\"n1-standard-4\")\n",
    "\n",
    "    # Save data to the output params\n",
    "    vertex_endpoint.uri = endpoint.resource_name\n",
    "    vertex_model.uri = deployed_model.resource_name\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "@dsl.pipeline(\n",
    "    # Default pipeline root. You can override it when submitting the pipeline.\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    # A name for the pipeline.\n",
    "    name=\"pipeline-test\",\n",
    "    description='Propensity BQML Test'\n",
    ")\n",
    "def pipeline():\n",
    "    \n",
    "    \n",
    "    create_input_view_op = create_input_view(view_name = VIEW_NAME,\n",
    "                                             data_set_id = DATA_SET_ID,\n",
    "                                             project_id = PROJECT_ID,\n",
    "                                             bucket_name = BUCKET_NAME,\n",
    "                                             blob_path = BLOB_PATH\n",
    "                                             )\n",
    "    \n",
    "\n",
    "    build_bqml_logistic_op = build_bqml_logistic(project_id = PROJECT_ID, \n",
    "                                                   data_set_id = DATA_SET_ID, \n",
    "                                                   model_name = 'bqml_logistic_model', \n",
    "                                                   training_view = VIEW_NAME\n",
    "                                                   )\n",
    "    \n",
    "    build_bqml_xgboost_op = build_bqml_xgboost(project_id = PROJECT_ID, \n",
    "                                                 data_set_id = DATA_SET_ID, \n",
    "                                                 model_name = 'bqml_xgboost_model', \n",
    "                                                 training_view = VIEW_NAME\n",
    "                                                 )\n",
    "    \n",
    "    build_bqml_automl_op = build_bqml_automl (project_id = PROJECT_ID, \n",
    "                                                data_set_id = DATA_SET_ID, \n",
    "                                                model_name = 'bqml_automl_model', \n",
    "                                                training_view = VIEW_NAME\n",
    "                                               )\n",
    "    \n",
    "    \n",
    "    \n",
    "    build_xgb_xgboost_op = build_xgb_xgboost(project_id = PROJECT_ID, \n",
    "                                                         data_set_id = DATA_SET_ID, \n",
    "                                                         training_view = VIEW_NAME\n",
    "                                                        )  \n",
    "       \n",
    "    \n",
    "    evaluate_bqml_logistic_op = evaluate_bqml_logistic(project_id = PROJECT_ID, \n",
    "                                                         data_set_id = DATA_SET_ID, \n",
    "                                                         model_name = 'bqml_logistic_model', \n",
    "                                                         training_view = VIEW_NAME\n",
    "                                                         )\n",
    "    \n",
    "    evaluate_bqml_xgboost_op = evaluate_bqml_xgboost(project_id = PROJECT_ID, \n",
    "                                                         data_set_id = DATA_SET_ID, \n",
    "                                                         model_name = 'bqml_xgboost_model', \n",
    "                                                         training_view = VIEW_NAME\n",
    "                                                         )\n",
    "    \n",
    "    evaluate_bqml_automl_op = evaluate_bqml_automl(project_id = PROJECT_ID, \n",
    "                                                         data_set_id = DATA_SET_ID, \n",
    "                                                         model_name = 'bqml_automl_model', \n",
    "                                                         training_view = VIEW_NAME\n",
    "                                                         )\n",
    "    \n",
    "    \n",
    "    deploy_xgb_op = deploy_xgb(project_id = PROJECT_ID,\n",
    "                                   model=build_xgb_xgboost_op.outputs[\"model\"]\n",
    "                                  ) \n",
    "                                                         \n",
    "                                \n",
    "    build_bqml_logistic_op.after(create_input_view_op)\n",
    "    build_bqml_xgboost_op.after(create_input_view_op)\n",
    "    build_bqml_automl_op.after(create_input_view_op)\n",
    "    build_xgb_xgboost_op.after(create_input_view_op)\n",
    "    \n",
    "    evaluate_bqml_logistic_op.after(build_bqml_logistic_op)\n",
    "    evaluate_bqml_xgboost_op.after(build_bqml_xgboost_op)\n",
    "    evaluate_bqml_automl_op.after(build_bqml_automl_op)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=\"pipeline.json\"\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "run = pipeline_jobs.PipelineJob(\n",
    "    display_name=\"test-pipeine\",\n",
    "    template_path=\"pipeline.json\",\n",
    "    \n",
    "    job_id=\"test-{0}\".format(TIMESTAMP),\n",
    "    enable_caching=False\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "run.run()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/449379755990/locations/us-central1/pipelineJobs/test-20210930193956\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/449379755990/locations/us-central1/pipelineJobs/test-20210930193956')\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/test-20210930193956?project=449379755990\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/449379755990/locations/us-central1/pipelineJobs/test-20210930193956 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/449379755990/locations/us-central1/pipelineJobs/test-20210930193956 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/449379755990/locations/us-central1/pipelineJobs/test-20210930193956 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/449379755990/locations/us-central1/pipelineJobs/test-20210930193956 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/449379755990/locations/us-central1/pipelineJobs/test-20210930193956 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/449379755990/locations/us-central1/pipelineJobs/test-20210930193956 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/449379755990/locations/us-central1/pipelineJobs/test-20210930193956 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Job failed with:\ncode: 9\nmessage: \"The DAG failed because some tasks failed. The failed tasks are: [evaluate-bqml-automl, evaluate-bqml-xgboost].; Job (project_id = tai-demo-experimental-gke, job_id = 6757132271381118976) is failed due to the above error.; Failed to handle the job: {project_number = 449379755990, job_id = 6757132271381118976}\"\n",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-7fb00ad6902e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/aiplatform/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    662\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m             \u001b[0;31m# callbacks to call within the Future (in same Thread)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/aiplatform/pipeline_jobs.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, service_account, network, sync)\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0m_LOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"View Pipeline Job:\\n%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dashboard_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_block_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/aiplatform/pipeline_jobs.py\u001b[0m in \u001b[0;36m_block_until_complete\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# JOB_STATE_FAILED or JOB_STATE_CANCELLED.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gca_resource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_PIPELINE_ERROR_STATES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Job failed with:\\n%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gca_resource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0m_LOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_action_completed_against_resource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Job failed with:\ncode: 9\nmessage: \"The DAG failed because some tasks failed. The failed tasks are: [evaluate-bqml-automl, evaluate-bqml-xgboost].; Job (project_id = tai-demo-experimental-gke, job_id = 6757132271381118976) is failed due to the above error.; Failed to handle the job: {project_number = 449379755990, job_id = 6757132271381118976}\"\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# this schedules a cron like job by building an endpoint using cloud functions and then scheduler\n",
    "\n",
    "from kfp.v2.google.client import AIPlatformClient\n",
    "\n",
    "api_client = AIPlatformClient(project_id=PROJECT_ID,\n",
    "                             region='us-central1'\n",
    "                             )\n",
    "\n",
    "api_client.create_schedule_from_job_spec(\n",
    "    job_spec_path='pipeline.json',\n",
    "    schedule='0 * * * *',\n",
    "    enable_caching=False\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "##################\n",
    "# Junkyard"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "\n",
    "    # The ID of your GCS bucket\n",
    "    # bucket_name = \"your-bucket-name\"\n",
    "    \n",
    "    \n",
    "    # The path to your file to upload\n",
    "    # source_file_name = \"local/path/to/file\"\n",
    "    # The ID of your GCS object\n",
    "    # destination_blob_name = \"storage-object-name\"\n",
    "    \n",
    "    \n",
    "#     storage_client = storage.Client()\n",
    "#     bucket = storage_client.bucket(bucket_name)\n",
    "#     blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "#     blob.upload_from_filename(source_file_name)\n",
    "    \n",
    "client = storage.Client()\n",
    "bucket = storage_client.bucket('propensity_model_assets')\n",
    "blob = bucket.blob('pipeline_root/449379755990/test-20210909214717/evaluate-model-logistic_-1619527851155914752')\n",
    "blob.upload_from_filename('image.png')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import io\n",
    "from google.cloud import storage\n",
    "\n",
    "client = storage.Client()\n",
    "bucket = client.bucket('propensity_model_assets')\n",
    "blob = bucket.blob('pipeline_root/449379755990/test-20210910193817/evaluate-model-xgboost_-6068239858067832832/xgboost_image_path/your-filename.png')\n",
    "\n",
    "# temporarily save image to buffer\n",
    "buf = io.BytesIO()\n",
    "graph.savefig(buf, format='png')\n",
    "\n",
    "# upload buffer contents to gcs\n",
    "blob.upload_from_string(\n",
    "    buf.getvalue(),\n",
    "    content_type='image/png')\n",
    "\n",
    "buf.close()\n",
    "\n",
    "# gcs url to uploaded matplotlib image\n",
    "url = blob.public_url"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "url"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "https://pantheon.corp.google.com/storage/browser/_details/propensity_model_assets/pipeline_root/449379755990/test-20210910193817/evaluate-model-xgboost_-6068239858067832832/xgboost_image_path;tab=live_object?project=tai-demo-experimental-gke"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "graph"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket('propensity_model_assets')\n",
    "blob = bucket.blob(\"pipeline_root/449379755990/test-20210909205951/evaluate-model-xgboost_7096063327712837632/test_image.png\")\n",
    "blob.upload_from_filename(\"image.png\")\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "https://pantheon.corp.google.com/storage/browser/propensity_model_assets/pipeline_root/449379755990/test-20210909205951/evaluate-model-xgboost_7096063327712837632;tab=objects?project=tai-demo-experimental-gke&prefix=&forceOnObjectsSortingFiltering=false"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "create_input_view('test', 'propensity', 'tai-demo-experimental-gke')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from google.cloud import bigquery\n",
    "import xgboost as xgb\n",
    "# from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "client = bigquery.Client()\n",
    "\n",
    "data_set = \"tai-demo-experimental-gke.propensity.test\"\n",
    "build_df_for_xgboost = '''\n",
    "SELECT * FROM `{data_set}`\n",
    "'''.format(data_set = data_set)\n",
    "\n",
    "job_config = bigquery.QueryJobConfig()\n",
    "df = client.query(build_df_for_xgboost, job_config=job_config).to_dataframe()  # Make an API request.\n",
    "df = pd.get_dummies(df.drop(['fullVisitorId'], axis=1), prefix=['visited_dma', 'visited_daypart', 'visited_dow'])\n",
    "\n",
    "\n",
    "X = df.drop(['label'], axis=1).values\n",
    "y = df['label'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test  = train_test_split(X,y)\n",
    "train = xgb.DMatrix(X_train, label=y_train)\n",
    "test = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "param = {\n",
    "    'reg_lambda':1,\n",
    "    'gamma':0,\n",
    "    'max_depth':3,\n",
    "    'objective': 'binary:hinge',\n",
    "    'eta': .2\n",
    "}\n",
    "epochs = 3\n",
    "\n",
    "model = xgb.train(param, train, epochs)\n",
    "predictions = model.predict(test)\n",
    "score = accuracy_score(y_test, predictions)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "score"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# from kfp.v2.google.client import AIPlatformClient\n",
    "\n",
    "# api_client = AIPlatformClient(project_id=PROJECT_ID,\n",
    "#                            region=REGION)\n",
    "\n",
    "# api_client.create_schedule_from_job_spec(\n",
    "#     job_spec_path=COMPILED_PIPELINE_PATH,\n",
    "#     schedule=SCHEDULE,\n",
    "#     time_zone=TIME_ZONE,\n",
    "#     parameter_values=PIPELINE_PARAMETERS\n",
    "# )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "project_id = 'tai-demo-experimental-gke'\n",
    "data_set_id = 'propensity'\n",
    "model_name = 'bqml_xgboost_model' \n",
    "training_view = 'test'\n",
    "\n",
    "\n",
    "from google.cloud import bigquery\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_recall_curve\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from joblib import dump\n",
    "import pandas as pd\n",
    "import pyarrow\n",
    "\n",
    "\n",
    "client = bigquery.Client(project=project_id)\n",
    "\n",
    "data_set = project_id+'.'+data_set_id+'.'+training_view\n",
    "build_df_for_xgboost = '''\n",
    "SELECT * FROM `{data_set}`\n",
    "'''.format(data_set = data_set)\n",
    "\n",
    "job_config = bigquery.QueryJobConfig()\n",
    "df = client.query(build_df_for_xgboost, job_config=job_config).to_dataframe()  # Make an API request.\n",
    "df = pd.get_dummies(df.drop(['fullVisitorId'], axis=1), prefix=['visited_dma', 'visited_daypart', 'visited_dow'])\n",
    "\n",
    "\n",
    "X = df.drop(['label'], axis=1).values\n",
    "y = df['label'].values\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test  = train_test_split(X,y)\n",
    "train = xgb.DMatrix(X_train, label=y_train)\n",
    "test = xgb.DMatrix(X_test, label=y_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "params = {\n",
    "    'reg_lambda':[0,1],\n",
    "    'gamma': [1, 1.5, 2, 2.5, 3],\n",
    "    'max_depth':[2,3,4,5,10,20],\n",
    "    'learning_rate': [.1,.01]\n",
    "    \n",
    "}\n",
    "\n",
    "# params = {\n",
    "#     'reg_lambda':[1],\n",
    "#     'gamma': [1, 1.5, 2, 2.5, 3],\n",
    "#     'max_depth':[2,3,4,5,6],\n",
    "#     'eta': [.2,.3]\n",
    "    \n",
    "# }\n",
    "# epochs = 2\n",
    "#     'objective': 'binary:hinge',\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# estimator = XGBClassifier(\n",
    "#     objective= 'binary:logistic',\n",
    "#     nthread=4,\n",
    "#     seed=42\n",
    "# )\n",
    "\n",
    "# grid_search = GridSearchCV(\n",
    "#     estimator=estimator,\n",
    "#     param_grid=params,\n",
    "#     scoring = 'roc_auc',\n",
    "#     n_jobs = 2,\n",
    "#     cv = 2,\n",
    "#     verbose=True\n",
    "# )\n",
    "\n",
    "# grid_search.fit(X, y)\n",
    "\n",
    "\n",
    "# model = XGBClassifier(n_estimators=600, objective='binary:hinge',\n",
    "#                     silent=True, nthread=1, use_label_encoder=False,\n",
    "#                          eval_metric=\"error\")\n",
    "\n",
    "model = XGBClassifier(n_estimators=50, objective='binary:hinge',\n",
    "                    silent=True, nthread=1,\n",
    "                         eval_metric=\"auc\")\n",
    "\n",
    "\n",
    "\n",
    "folds = 3\n",
    "param_comb = 5\n",
    "\n",
    "skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n",
    "\n",
    "random_search = RandomizedSearchCV(model, param_distributions=params, \n",
    "                                   n_iter=param_comb, scoring='precision', \n",
    "                                   n_jobs=4, cv=skf.split(X_train,y_train), verbose=3, \n",
    "                                   random_state=1001 )\n",
    "\n",
    "\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "model = random_search.best_estimator_\n",
    "auc = random_search.best_score_\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "score = accuracy_score(y_test, predictions)\n",
    "auc = roc_auc_score(y_test, predictions)\n",
    "precision_recall = precision_recall_curve(y_test, predictions)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "score"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "precision_recall"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "random_search.best_estimator_"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "project_id = 'tai-demo-experimental-gke'\n",
    "data_set_id = 'propensity'\n",
    "model_name = 'bqml_xgboost_model' \n",
    "training_view = 'test'\n",
    "\n",
    "from google.cloud import bigquery\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_recall_curve\n",
    "from joblib import dump\n",
    "import pandas as pd\n",
    "import pyarrow\n",
    "\n",
    "client = bigquery.Client(project=project_id)\n",
    "\n",
    "data_set = project_id+'.'+data_set_id+'.'+training_view\n",
    "build_df_for_xgboost = '''\n",
    "SELECT * FROM `{data_set}`\n",
    "'''.format(data_set = data_set)\n",
    "\n",
    "job_config = bigquery.QueryJobConfig()\n",
    "df = client.query(build_df_for_xgboost, job_config=job_config).to_dataframe()  # Make an API request.\n",
    "df = pd.get_dummies(df.drop(['fullVisitorId'], axis=1), prefix=['visited_dma', 'visited_daypart', 'visited_dow'])\n",
    "\n",
    "\n",
    "X = df.drop(['label'], axis=1).values\n",
    "y = df['label'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test  = train_test_split(X,y)\n",
    "train = xgb.DMatrix(X_train, label=y_train)\n",
    "test = xgb.DMatrix(X_test, label=y_test)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "params = {\n",
    "    'reg_lambda':[1],\n",
    "    'gamma': [1, 1.5],\n",
    "    'max_depth':[2,3,4],\n",
    "    'eta': [.2]\n",
    "}\n",
    "\n",
    "model = XGBClassifier(learning_rate=0.02, n_estimators=600, objective='binary:logistic',\n",
    "                    silent=True, nthread=1, use_label_encoder=False,\n",
    "                         eval_metric=\"error\")\n",
    "\n",
    "folds = 3\n",
    "param_comb = 5\n",
    "\n",
    "skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n",
    "\n",
    "random_search = RandomizedSearchCV(model, param_distributions=params, \n",
    "                                   n_iter=param_comb, scoring='precision', \n",
    "                                   n_jobs=4, cv=skf.split(X_train,y_train), verbose=3, \n",
    "                                   random_state=1001 )\n",
    "\n",
    "\n",
    "random_search.fit(X_train, y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = random_search.best_estimator_\n",
    "predictions = model.predict(X_test)\n",
    "score = accuracy_score(y_test, predictions)\n",
    "auc = roc_auc_score(y_test, predictions)\n",
    "precision_recall = precision_recall_curve(y_test, predictions)\n",
    "\n",
    "# metrics.log_metric(\"accuracy\",(score * 100.0))\n",
    "# metrics.log_metric(\"framework\", \"xgboost\")\n",
    "# metrics.log_metric(\"dataset_size\", len(df))\n",
    "# metrics.log_metric(\"AUC\", auc)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.save_model(\"model.joblib\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dump(model, \"model.joblib\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# def deploy_model(\n",
    "#     model: Input[Model],\n",
    "#     project_id: str,\n",
    "#     vertex_endpoint: Output[Artifact],\n",
    "#     vertex_model: Output[Model]\n",
    "# ):\n",
    "    \n",
    "    \n",
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=project_id)\n",
    "#aiplatform.init(project=project, location=region)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "deployed_model = aiplatform.Model.upload(\n",
    "display_name=\"tai-propensity-test-pipeline\",\n",
    "artifact_uri = model.uri.replace(\"model_path\", \"\"),\n",
    "serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/xgboost-cpu.1-4:latest\"\n",
    ")\n",
    "endpoint = deployed_model.deploy(machine_type=\"n1-standard-4\")\n",
    "\n",
    "# Save data to the output params\n",
    "vertex_endpoint.uri = endpoint.resource_name\n",
    "vertex_model.uri = deployed_model.resource_name"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "project_id = 'tai-demo-experimental-gke'\n",
    "data_set_id = 'propensity'\n",
    "model_name = 'bqml_xgboost_model' \n",
    "training_view = 'test'\n",
    "\n",
    "\n",
    "from google.cloud import bigquery\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_recall_curve\n",
    "from joblib import dump\n",
    "import pandas as pd\n",
    "import pyarrow\n",
    "\n",
    "client = bigquery.Client(project=project_id)\n",
    "\n",
    "data_set = project_id+'.'+data_set_id+'.'+training_view\n",
    "build_df_for_xgboost = '''\n",
    "SELECT * FROM `{data_set}`\n",
    "'''.format(data_set = data_set)\n",
    "\n",
    "job_config = bigquery.QueryJobConfig()\n",
    "df = client.query(build_df_for_xgboost, job_config=job_config).to_dataframe()  # Make an API request.\n",
    "df = pd.get_dummies(df.drop(['fullVisitorId'], axis=1), prefix=['visited_dma', 'visited_daypart', 'visited_dow'])\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "X = df.drop(['label'], axis=1).values\n",
    "y = df['label'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test  = train_test_split(X,y)\n",
    "train = xgb.DMatrix(X_train, label=y_train)\n",
    "test = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "params = {\n",
    "    'reg_lambda':[1],\n",
    "    'gamma': [1, 1.5],\n",
    "    'max_depth':[2,3,4],\n",
    "    'eta': [.2]\n",
    "}\n",
    "\n",
    "xgb_model = XGBClassifier(learning_rate=0.02, n_estimators=600, objective='binary:logistic',\n",
    "                    silent=True, nthread=1, use_label_encoder=False,\n",
    "                         eval_metric=\"error\")\n",
    "\n",
    "folds = 3\n",
    "param_comb = 5\n",
    "\n",
    "skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n",
    "\n",
    "random_search = RandomizedSearchCV(xgb_model, param_distributions=params, \n",
    "                                   n_iter=param_comb, scoring='precision', \n",
    "                                   n_jobs=4, cv=skf.split(X_train,y_train), verbose=3, \n",
    "                                   random_state=1001 )\n",
    "\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "xgb_model_best = random_search.best_estimator_\n",
    "predictions = xgb_model_best.predict(X_test)\n",
    "score = accuracy_score(y_test, predictions)\n",
    "auc = roc_auc_score(y_test, predictions)\n",
    "precision_recall = precision_recall_curve(y_test, predictions)\n",
    "\n",
    "# metrics.log_metric(\"accuracy\",(score * 100.0))\n",
    "# metrics.log_metric(\"framework\", \"xgboost\")\n",
    "# metrics.log_metric(\"dataset_size\", len(df))\n",
    "# metrics.log_metric(\"AUC\", auc)\n",
    "\n",
    "dump(xgb_model_best, \"model_path\" + \".joblib\")\n",
    "#model.save_model(model_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m76",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m76"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}